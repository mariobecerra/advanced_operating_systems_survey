@inproceedings{Chiba2016,
abstract = {Besides being an in-memory oriented computing framework, Spark runs on top of a Java Virtual Machine (JVM), so JVM parameters must be tuned to improve Spark application performance. Misconfigured parameters and set- tings degrade performance. For example, using Java heaps that are too large often causes long garbage collection pause time, which accounts for over 10-20{\%} of application execution time. Moreover, recent modern computing nodes have many cores and support running multiple threads simultaneously with SMT technology. Thus, optimization in full stack is also important. Not only JVM parameters but also OS parameters, Spark configuration, and application code itself based on CPU characteristics need to be optimized to take full advantage of underlying computing resource. In this paper, we use TPC-H benchmark as our optimization case study and gather many perspective logs such as application log, JVM log such as GC and JIT, system utilization, and hardware events from PMU. We investigate the existing problems and then introduce several optimization approaches for accelerating Spark performance. As a result, our optimization achieves 30 - 40{\%} speed up on average, and up to 5x faster than the naive configuration.},
author = {Chiba, Tatsuhiro and Onodera, Tamiya},
booktitle = {ISPASS 2016 - International Symposium on Performance Analysis of Systems and Software},
doi = {10.1109/ISPASS.2016.7482079},
isbn = {9781509019526},
title = {{Workload characterization and optimization of TPC-H queries on Apache Spark}},
year = {2016}
}

@article{Marco2017,
abstract = {Data analytic applications built upon big data processing frameworks such as Apache Spark are an important class of applications. Many of these applications are not latency-sensitive and thus can run as batch jobs in data centers. By running multiple applications on a computing host, task co-location can significantly improve the server utilization and system throughput. However, effective task co-location is a non-trivial task, as it requires an understanding of the computing resource requirement of the co-running applications, in order to determine what tasks, and how many of them, can be co-located. In this paper, we present a mixture-of-experts approach to model the memory behavior of Spark applications. We achieve this by learning, off-line, a range of specialized memory models on a range of typical applications; we then determine at runtime which of the memory models, or experts, best describes the memory behavior of the target application. We show that by accurately estimating the resource level that is needed, a co-location scheme can effectively determine how many applications can be co-located on the same host to improve the system throughput, by taking into consideration the memory and CPU requirements of co-running application tasks. Our technique is applied to a set of representative data analytic applications built upon the Apache Spark framework. We evaluated our approach for system throughput and average normalized turnaround time on a multi-core cluster. Our approach achieves over 83.9{\%} of the performance delivered using an ideal memory predictor. We obtain, on average, 8.69x improvement on system throughput and a 49{\%} reduction on turnaround time over executing application tasks in isolation, which translates to a 1.28x and 1.68x improvement over a state-of-the-art co-location scheme for system throughput and turnaround time respectively.},
archivePrefix = {arXiv},
arxivId = {1710.00610},
author = {Marco, Vicent Sanz and Taylor, Ben and Porter, Barry and Wang, Zheng},
eprint = {1710.00610},
isbn = {9781450347204},
title = {{Improving Spark Application Throughput Via Memory Aware Task Co-location: A Mixture of Experts Approach}},
year = {2017}
}

@inproceedings{Chaimov2016,
abstract = {Copyright ? 2016 by the Association for Computing Machinery, Inc. (ACM).We report our experiences porting Spark to large production HPC systems. While Spark performance in a data center installation (with local disks) is dominated by the network, our results show that file system metadata access latency can dominate in a HPC installation using Lustre: it determines single node performance up to 4? slower than a typical workstation. We evaluate a combination of software techniques and hardware configurations designed to address this problem. For example, on the software side we develop a file pooling layer able to improve per node performance up to 2.8?. On the hardware side we evaluate a system with a large NVRAM buffer between compute nodes and the backend Lustre file system: this improves scaling at the expense of per-node performance. Overall, our results indicate that scalability is currently limited to O(102) cores in a HPC installation with Lustre and default Spark. After careful configuration combined with our pooling we can scale up to O(104). As our analysis indicates, it is feasible to observe much higher scalability in the near future.},
author = {Chaimov, Nicholas and Malony, Allen and Canon, Shane and Iancu, Costin and Ibrahim, Khaled Z. and Srinivasan, Jay},
booktitle = {Proceedings of the 25th ACM International Symposium on High-Performance Parallel and Distributed Computing - HPDC '16},
doi = {10.1145/2907294.2907310},
isbn = {9781450343145},
pmid = {202877},
title = {{Scaling Spark on HPC Systems}},
year = {2016}
}

@article{kim2017sparkle,
  title={Sparkle: Optimizing Spark for Large Memory Machines and Analytics},
  author={Kim, Mijung and Li, Jun and Volos, Haris and Marwah, Manish and Ulanov, Alexander and Keeton, Kimberly and Tucek, Joseph and Cherkasova, Lucy and Xu, Le and Fernando, Pradeep},
  journal={arXiv preprint arXiv:1708.05746},
  year={2017}
}

@inproceedings{Wang2016,
author = {Wang, Li and Xu, Tianni and Wang, Jing and Zhang, Weigong and Sui, Xiufeng and Bao, Yungang},
booktitle = {Proceedings of the Posters and Demos Session of the 17th International Middleware Conference on ZZZ - Middleware Posters and Demos '16},
doi = {10.1145/3007592.3007593},
isbn = {9781450346665},
title = {{Understanding the Behavior of Spark Workloads from Linux Kernel Parameters Perspective}},
year = {2016}
}

@inproceedings{Shvachko2010,
abstract = {The Hadoop Distributed File System (HDFS) is designed to store very large data sets reliably, and to stream those data sets at high bandwidth to user applications. In a large cluster, thousands of servers both host directly attached storage and execute user application tasks. By distributing storage and computation across many servers, the resource can grow with demand while remaining economical at every size. We describe the architecture of HDFS and report on experience using HDFS to manage 25 petabytes of enterprise data at Yahoo!.},
author = {Shvachko, Konstantin and Kuang, Hairong and Radia, Sanjay and Chansler, Robert},
booktitle = {2010 IEEE 26th Symposium on Mass Storage Systems and Technologies, MSST2010},
doi = {10.1109/MSST.2010.5496972},
isbn = {9781424471539},
issn = {978-1-4244-7152-2},
keywords = {Distributed file system,HDFS,Hadoop},
title = {{The Hadoop distributed file system}},
year = {2010}
}

@inproceedings{armbrust2015spark,
  title={Spark sql: Relational data processing in spark},
  author={Armbrust, Michael and Xin, Reynold S and Lian, Cheng and Huai, Yin and Liu, Davies and Bradley, Joseph K and Meng, Xiangrui and Kaftan, Tomer and Franklin, Michael J and Ghodsi, Ali and others},
  booktitle={Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data},
  pages={1383--1394},
  year={2015},
  organization={ACM}
}


@article{zaharia_spark:_2010,
  title = {Spark: {Cluster} computing with working sets.},
  volume = {10},
  shorttitle = {Spark},
  url = {http://static.usenix.org/legacy/events/hotcloud10/tech/full_papers/Zaharia.pdf},
  number = {10-10},
  urldate = {2017-10-13},
  journal = {HotCloud},
  author = {Zaharia, Matei and Chowdhury, Mosharaf and Franklin, Michael J. and Shenker, Scott and Stoica, Ion},
  year = {2010},
  pages = {95},
  }

@inproceedings{zaharia_resilient_2012,
  title = {Resilient distributed datasets: {A} fault-tolerant abstraction for in-memory cluster computing},
  shorttitle = {Resilient distributed datasets},
  url = {http://dl.acm.org/citation.cfm?id=2228301},
  urldate = {2017-10-13},
  booktitle = {Proceedings of the 9th {USENIX} conference on {Networked} {Systems} {Design} and {Implementation}},
  publisher = {USENIX Association},
  author = {Zaharia, Matei and Chowdhury, Mosharaf and Das, Tathagata and Dave, Ankur and Ma, Justin and McCauley, Murphy and Franklin, Michael J. and Shenker, Scott and Stoica, Ion},
  year = {2012},
  pages = {2--2},
  }

@article{Armbrust2015,
abstract = {Apache Spark is one of the most widely used open source processing engines for big data, with rich language-integrated APIs and a wide range of libraries. Over the past two years, our group has worked to deploy Spark to a wide range of organizations through consulting relationships as well as our hosted service, Databricks. We describe the main challenges and requirements that appeared in taking Spark to a wide set of users, and usability and performance improvements we have made to the engine in response.},
author = {Armbrust, Michael and Zaharia, Matei and Das, Tathagata and Davidson, Aaron and Ghodsi, Ali and Or, Andrew and Rosen, Josh and Stoica, Ion and Wendell, Patrick and Xin, Reynold},
doi = {10.14778/2824032.2824080},
issn = {21508097},
journal = {Proceedings of the VLDB Endowment},
title = {{Scaling spark in the real world}},
year = {2015}
}

@inproceedings{Hema2016,
author = {Hema, N. and Srinivasa, K. G. and Chidambaram, Saravanan and Saraswat, Sandeep and Saraswati, Sujoy and Ramachandra, Ranganath and Huttanagoudar, Jayashree B.},
booktitle = {Proceedings of the International Conference on Informatics and Analytics - ICIA-16},
doi = {10.1145/2980258.2982117},
isbn = {9781450347563},
title = {{Performance Analysis of Java Virtual Machine for Machine Learning Workloads using Apache Spark}},
year = {2016}
}

@inproceedings{Islam2015,
abstract = {For data-intensive computing, the low throughput of the existing disk-bound storage systems is a major bottleneck. Recent emergence of the in-memory file systems with heterogeneous storage support mitigates this problem to a great extent. Parallel programming frameworks, e.g. Hadoop MapReduce and Spark are increasingly being run on such high-performance file systems. However, no comprehensive study has been done to analyze the impacts of the in-memory file systems on various Big Data applications. This paper characterizes two file systems in literature, Tachyon 17] and Triple-H 13] that support in-memory and heterogeneous storage, and discusses the impacts of these two architectures on the performance and fault tolerance of Hadoop MapReduce and Spark applications. We present a complete methodology for evaluating MapReduce and Spark workloads on top of in-memory file systems and provide insights about the interactions of different system components while running these workloads. We also propose advanced acceleration techniques to adapt Triple-H for iterative applications and study the impact of different parameters on the performance of MapReduce and Spark jobs on HPC systems. Our evaluations show that, although Tachyon is 5x faster than HDFS for primitive operations, Triple-H performs 47{\%} and 2.4x better than Tachyon for MapReduce and Spark workloads, respectively. Triple-H also accelerates K-Means by 15{\%} over HDFS and 9{\%} over Tachyon.},
author = {Islam, Nusrat Sharmin and Wasi-Ur-Rahman, Md and Lu, Xiaoyi and Shankar, Dipti and Panda, Dhabaleswar K.},
booktitle = {Proceedings - 2015 IEEE International Conference on Big Data, IEEE Big Data 2015},
doi = {10.1109/BigData.2015.7363761},
isbn = {9781479999255},
issn = {2332-7790},
title = {{Performance characterization and acceleration of in-memory file systems for Hadoop and Spark applications on HPC clusters}},
year = {2015}
}

@article{davidson2013optimizing,
  title={Optimizing shuffle performance in spark},
  author={Davidson, Aaron and Or, Andrew},
  journal={University of California, Berkeley-Department of Electrical Engineering and Computer Sciences, Tech. Rep},
  year={2013}
}

@inproceedings{Lu2013,
abstract = {Hadoop RPC is the basic communication mechanism in the Hadoop ecosystem. It is used with other Hadoop components like MapReduce, HDFS, and HBase in real world data-centers, e.g. Facebook and Yahoo!. However, the current Hadoop RPC design is built on ...},
author = {Lu, Xiaoyi and Islam, Nusrat S. and Md., Wasi Ur Rahman and Jose, Jithin and Subramoni, Hari and Wang, Hao and Panda, Dhabaleswar K.},
booktitle = {Proceedings of the International Conference on Parallel Processing},
doi = {10.1109/ICPP.2013.78},
isbn = {9780769551173},
issn = {01903918},
title = {{High-Performance design of hadoop RPC with RDMA over InfiniBand}},
year = {2013}
}

@article{Morris2017,
abstract = {Scale-out parallel processing based on MPI is a 25-year-old standard with at least another decade of preceding history of enabling technologies in the High Performance Comput-ing community. Newer frameworks such as MapReduce, Hadoop, and Spark represent industrial scalable comput-ing solutions that have received broad adoption because of their comparative simplicity of use, applicability to relevant problems, and ability to harness scalable, distributed re-sources. While MPI provides performance and portability, it lacks in productivity and fault tolerance. Likewise, Spark is a specific example of a current-generation MapReduce and data-parallel computing infrastructure that addresses those goals but in turn lacks peer communication support to al-low featherweight, highly scalable peer-to-peer data-parallel code sections. The key contribution of this paper is to demonstrate how to introduce the collective and point-to-point peer communica-tion concepts of MPI into a Spark environment. This is done in order to produce performance-portable, peer-oriented and group-oriented communication services while retaining the essential, desirable properties of Spark. Additional concepts of fault tolerance and productivity are considered. This ap-proach is offered in contrast to adding MapReduce frame-work as upper-middleware based on a traditional MPI im-plementation as baseline infrastructure.},
archivePrefix = {arXiv},
arxivId = {arXiv:1707.04788v1},
author = {Morris, Brandon L and Skjellum, Anthony},
eprint = {arXiv:1707.04788v1},
keywords = {MPI,Scala,Spark,data-parallel,parallel closures,peer-to-peer communication,task-parallel},
title = {{MPIgnite: An MPI-Like Language and Prototype Implementation for Apache Spark}},
year = {2017}
}

@inproceedings{Ulanov2017,
abstract = {Present day machine learning is computationally intensive and processes large amounts of data. It is implemented in a distributed fashion in order to address these scalability issues. The work is parallelized across a number of computing nodes. It is usually hard to estimate in advance how many nodes to use for a particular workload. We propose a simple framework for estimating the scalability of distributed machine learning algorithms. We measure the scalability by means of the speedup an algorithm achieves with more nodes. We propose time complexity models for gradient descent and graphical model inference. We validate our models with experiments on deep learning training and belief propagation. This framework was used to study the scalability of machine learning algorithms in Apache Spark.},
archivePrefix = {arXiv},
arxivId = {1610.06276},
author = {Ulanov, Alexander and Simanovsky, Andrey and Marwah, Manish},
booktitle = {Proceedings - International Conference on Data Engineering},
doi = {10.1109/ICDE.2017.160},
eprint = {1610.06276},
isbn = {9781509065431},
issn = {10844627},
title = {{Modeling scalability of distributed machine learning}},
year = {2017}
}

@article{Meng2016,
abstract = {Apache Spark is a popular open-source platform for large-scale data processing that is well-suited for iterative machine learning tasks. In this paper we present MLlib, Spark's open-source distributed machine learning library. MLlib provides efficient functionality for a wide range of learning settings and includes several underlying statistical, optimization, and linear algebra primitives. Shipped with Spark, MLlib supports several languages and provides a high-level API that leverages Spark's rich ecosystem to simplify the development of end-to-end machine learning pipelines. MLlib has experienced a rapid growth due to its vibrant open-source community of over 140 contributors, and includes extensive documentation to support further growth and to let users quickly get up to speed.},
author = {Meng, Xiangrui and Bradley, Joseph and Yavuz, Burak and Sparks, Evan and Venkataraman, Shivaram and Liu, Davies and Freeman, Jeremy and Tsai, DB and Amde, Manish and Owen, Sean and Xin, Doris and Xin, Reynold and Franklin, Michael J and Zadeh, Reza and Zaharia, Matei and Talwalkar, Ameet},
journal = {Journal of Machine Learning Research},
keywords = {apache spark,distributed algorithms,scalable machine learning},
pages = {1--7},
title = {{MLlib: Machine Learning in Apache Spark}},
volume = {17},
year = {2016}
}

@inproceedings{Lu2016,
abstract = {The in-memory data processing framework, Apache Spark, has been stealing the limelight for low-latency interactive applications, iterative and batch computations. Our early experience study [17] has shown that Apache Spark can be enhanced to leverage advanced features (e.g., RDMA) on high-performance networks (e.g., InfiniBand and RoCE) to improve the performance of shuffle phase. With the fast evolving of the Apache Spark ecosystem, the Spark architecture has been changing a lot. This motivates us to investigate whether the earlier RDMA design can be adapted and further enhanced for the new Apache Spark architecture. We also aim to improve the performance for various Spark workloads (e.g., Batch, Graph, SQL). In this paper, we present a detailed design for high-performance RDMA-based Apache Spark on high-performance networks. We conduct systematic performance evaluations on three modern clusters (Chameleon, SDSC Comet, and an in-house cluster) with cutting-edge InfiniBand technologies, such as latest IB EDR (100 Gbps) network, recently introduced Single Root I/O Virtualization (SR-IOV) technology for IB, etc. The evaluation results show that compared to the default Spark running with IP over InfiniBand (IPoIB), our proposed design can achieve up to 79{\%} performance improvement for Spark RDD operation benchmarks (e.g., GroupBy, SortBy), up to 38{\%} performance improvement for batch workloads (e.g., Sort and TeraSort in Intel HiBench), up to 46{\%} performance improvement for graph processing workloads (e.g., PageRank), up to 32{\%} performance improvement for SQL queries (e.g., Aggregation, Join) on varied scales (up to 1,536 cores) of bare-metal IB clusters. Performance evaluations on SR-IOV enabled IB clusters also show 37{\%} improvement achieved by our RDMA-based design. Our RDMA-based Spark design is implemented as a pluggable module and it does not change any Spark APIs, which means that it can be combined with other existing enhanced designs for Apache Spark and Hadoop - roposed in the community. To show this, we further evaluate the performance of a combined version of `RDMA-Spark+RDMA-HDFS' and the numbers show that the combination can achieve the best performance with up to 82{\%} improvement for Intel HiBench Sort and TeraSort on SDSC Comet cluster.},
author = {Lu, Xiaoyi and Shankar, Dipti and Gugnani, Shashank and Panda, Dhabaleswar K.D.K.},
booktitle = {Proceedings - 2016 IEEE International Conference on Big Data, Big Data 2016},
doi = {10.1109/BigData.2016.7840611},
isbn = {9781467390040},
pmid = {172550},
title = {{High-performance design of apache spark with RDMA and its benefits on various workloads}},
year = {2016}
}

@article{Zadeh,
abstract = {We describe matrix computations available in the cluster programming framework, Apache Spark. Out of the box, Spark provides abstractions and implementations for dis-tributed matrices and optimization routines using these ma-trices. When translating single-node algorithms to run on a distributed cluster, we observe that often a simple idea is enough: separating matrix operations from vector opera-tions and shipping the matrix operations to be ran on the cluster, while keeping vector operations local to the driver. In the case of the Singular Value Decomposition, by taking this idea to an extreme, we are able to exploit the computa-tional power of a cluster, while running code written decades ago for a single core. Another example is our Spark port of the popular TFOCS optimization package, originally built for MATLAB, which allows for solving Linear programs as well as a variety of other convex programs. We conclude with a comprehensive set of benchmarks for hardware accelerated matrix computations from the JVM, which is interesting in its own right, as many cluster programming frameworks use the JVM. The contributions described in this paper are al-ready merged into Apache Spark and available on Spark installations by default, and commercially supported by a slew of companies which provide further services.},
author = {Zadeh, Reza Bosagh and Meng, Xiangrui and Ulanov, Alexander and Yavuz, Burak and Pu, Li and Venkataraman, Shivaram and Sparks, Evan and Staple, Aaron and Zaharia, Matei},
doi = {10.1145/2939672.2939675},
keywords = {Concur-rent algorithms,Keywords Distributed Linear Algebra,MLlib,Machine Learning,Machine learning algorithms,Matrix Computations,Opti-mization,Solvers,Spark},
title = {{Matrix Computations and Optimization in Apache Spark}},
url = {http://dx.doi.org/10.1145/2939672.2939675}
}

@inproceedings{zaharia2013discretized,
  title={Discretized streams: Fault-tolerant streaming computation at scale},
  author={Zaharia, Matei and Das, Tathagata and Li, Haoyuan and Hunter, Timothy and Shenker, Scott and Stoica, Ion},
  booktitle={Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles},
  pages={423--438},
  year={2013},
  organization={ACM}
}

@inproceedings{Li2015,
abstract = {Analytics algorithms on big data sets require tremendous computational capabilities. Spark is a recent development that addresses big data challenges with data and computation distribution and in-memory caching. However, as a CPU only framework, Spark cannot leverage GPUs and a growing set of GPU libraries to achieve better performance and energy efficiency. We present HeteroSpark, a GPU-accelerated heterogeneous architecture integrated with Spark, which combines the massive compute power of GPUs and scalability of CPUs and system memory resources for applications that are both data and compute intensive. We make the following contributions in this work: (1) we integrate the GPU accelerator into current Spark framework to further leverage data parallelism and achieve algorithm acceleration; (2) we provide a plug-n-play design by augmenting Spark platform so that current Spark applications can choose to enable/disable GPU acceleration; (3) application acceleration is transparent to developers, therefore existing Spark applications can be easily ported to this heterogeneous platform without code modifications. The evaluation of HeteroSpark demonstrates up to 18{\&}{\#}x00D7; speedup on a number of machine learning applications.},
author = {Li, Peilong and Luo, Yan and Zhang, Ning and Cao, Yu},
booktitle = {Proceedings of the 2015 IEEE International Conference on Networking, Architecture and Storage, NAS 2015},
doi = {10.1109/NAS.2015.7255222},
isbn = {9781467378918},
keywords = {Acceleration,Big data,Computer architecture,Graphics processing units,Libraries,Machine learning algorithms,Sparks},
title = {{HeteroSpark: A heterogeneous CPU/GPU Spark platform for machine learning algorithms}},
year = {2015}
}

@inproceedings{gonzalez2014graphx,
  title={GraphX: Graph Processing in a Distributed Dataflow Framework.},
  author={Gonzalez, Joseph E and Xin, Reynold S and Dave, Ankur and Crankshaw, Daniel and Franklin, Michael J and Stoica, Ion},
  booktitle={OSDI},
  volume={14},
  pages={599--613},
  year={2014}
}

@inproceedings{Luckow2016,
abstract = {High performance distributed computing environments have traditionally been designed to meet the compute demands of scientific applications, supercomputers have historically been producers and not consumers of data. The Apache Hadoop ecosystem has evolved to address many of the traditional limitations of HPC platforms. There exist a whole class of scientific applications that need the collective capabilities of traditional high-performance computing environments and the Apache Hadoop ecosystem. For example, the scientific domains of bio-molecular dynamics, genomics and high-energy physics need to couple traditional computing with Hadoop/Spark based analysis. We investigate the critical question of how to present both capabilities to such scientific applications. Whereas this questions needs answers at multiple levels, we focus on the design of middleware that might support the needs of both. We propose extensions to the Pilot-Abstraction so as to provide a unifying resource management layer. This provides an important step towards integration and thereby interoperable use of HPC and Hadoop/Spark, and allows applications to efficiently couple HPC stages (e.g. simulations) to data analytics. Many supercomputing centers have started to officially support Hadoop environments either in a dedicated environment or in hybrid deployments using tools, such as myHadoop. However, this typically involves many intrinsic, environment-specific details that need to be mastered, and often swamp conceptual questions like: How best to couple HPC and Hadoop application stages? How to explore runtime trade-offs (data localities vs. data movement)? This paper provides both conceptual understanding and practical solutions to questions central to the integrated use of HPC and Hadoop environments. Our experiments are performed on state-of-the-art production HPC environments and provide middleware for multiple domain sciences.},
archivePrefix = {arXiv},
arxivId = {1602.00345},
author = {Luckow, Andre and Paraskevakos, Ioannis and Chantzialexiou, George and Jha, Shantenu},
booktitle = {Proceedings - 2016 IEEE 30th International Parallel and Distributed Processing Symposium, IPDPS 2016},
doi = {10.1109/IPDPSW.2016.166},
eprint = {1602.00345},
isbn = {9781509021406},
keywords = {Big Data,HPC,Hadoop},
title = {{Hadoop on HPC: Integrating hadoop and pilot-based dynamic resource management}},
year = {2016}
}

@inproceedings{Lu2014,
abstract = {Apache Hadoop Map Reduce has been highly successful in processing large-scale, data-intensive batch applications on commodity clusters. However, for low-latency interactive applications and iterative computations, Apache Spark, an emerging in-memory processing framework, has been stealing the limelight. Recent studies have shown that current generation Big Data frameworks (like Hadoop) cannot efficiently leverage advanced features (e.g. RDMA) on modern clusters with high-performance networks. One of the major bottlenecks is that these middleware are traditionally written with sockets and do not deliver the best performance on modern HPC systems with RDMA-enabled high-performance interconnects. In this paper, we first assess the opportunities of bringing the benefits of RDMA into the Spark framework. We further propose a high-performance RDMA-based design for accelerating data shuffle in the Spark framework on high-performance networks. Performance evaluations show that our proposed design can achieve 79-83{\%} performance improvement for Group By, compared with the default Spark running with IP over Infini Band (IPoIB) FDR on a 128-256 core cluster. We adopt a plug-in-based approach that can make our design to be easily integrated with newer Spark releases. To the best our knowledge, this is the first design for accelerating Spark with RDMA for Big Data processing. {\textcopyright} 2014 IEEE.},
author = {Lu, Xiaoyi and Rahman, Md Wasi Ur and Islam, Nusrat and Shankar, Dipti and Panda, Dhabaleswar K.},
booktitle = {Proceedings - 2014 IEEE 22nd Annual Symposium on High-Performance Interconnects, HOTI 2014},
doi = {10.1109/HOTI.2014.15},
isbn = {9781479958603},
issn = {1550-4794},
keywords = {Apache Spark,InfiniBand,RDMA},
title = {{Accelerating spark with RDMA for big data processing: Early experiences}},
year = {2014}
}

@inproceedings{Islam2016,
author = {Islam, Nusrat Sharmin and Wasi-Ur-Rahman, Md and Lu, Xiaoyi and Panda, Dhabaleswar K.D.K.},
booktitle = {Proceedings - 2016 IEEE International Conference on Big Data, Big Data 2016},
doi = {10.1109/BigData.2016.7840608},
isbn = {9781467390040},
title = {{Efficient data access strategies for Hadoop and Spark on HPC cluster with heterogeneous storage}},
year = {2016}
}

@article{zaharia_apache_2016,
  title = {Apache {Spark}: a unified engine for big data processing},
  volume = {59},
  issn = {00010782},
  shorttitle = {Apache {Spark}},
  url = {http://dl.acm.org/citation.cfm?doid=3013530.2934664},
  doi = {10.1145/2934664},
  language = {en},
  number = {11},
  urldate = {2017-10-13},
  journal = {Communications of the ACM},
  author = {Zaharia, Matei and Franklin, Michael J. and Ghodsi, Ali and Gonzalez, Joseph and Shenker, Scott and Stoica, Ion and Xin, Reynold S. and Wendell, Patrick and Das, Tathagata and Armbrust, Michael and Dave, Ankur and Meng, Xiangrui and Rosen, Josh and Venkataraman, Shivaram},
  month = oct,
  year = {2016},
  pages = {56--65},
  }

@misc{SparkPackages,
  title = {A community index of third-party packages for Apache Spark},
  howpublished = {\url{https://spark-packages.org/}},
  note = {Accessed: 2017-11-04}
}

@article{pfister2001introduction,
  title={An introduction to the infiniband architecture},
  author={Pfister, Gregory F},
  journal={High Performance Mass Storage and Parallel I/O},
  volume={42},
  pages={617--632},
  year={2001},
  publisher={chapter42}
}

@inproceedings{wang2015performance,
  title={Performance prediction for apache spark platform},
  author={Wang, Kewen and Khan, Mohammad Maifi Hasan},
  booktitle={High Performance Computing and Communications (HPCC), 2015 IEEE 7th International Symposium on Cyberspace Safety and Security (CSS), 2015 IEEE 12th International Conferen on Embedded Software and Systems (ICESS), 2015 IEEE 17th International Conference on},
  pages={166--173},
  year={2015},
  organization={IEEE}
}

@misc{TPC_H_benchmark,
  title = {TPC-H benchmark},
  howpublished = {\url{http://www.tpc.org/tpch/}},
  note = {Accessed: 2017-11-05}
}









@inproceedings{Xu2016,
  abstract = {—Memory is a crucial resource for big data processing frameworks such as Spark and M3R, where the memory is used both for computation and for caching intermediate storage data. Consequently, optimizing memory is the key to extracting high performance. The extant approach is to statically split the memory for computation and caching based on workload pro-filing. This approach is unable to capture the varying workload characteristics and dynamic memory demands. Another factor that affects caching efficiency is the choice of data placement and eviction policy. The extant LRU policy is oblivious of task scheduling information from the analytic frameworks, and thus can lead to lost optimization opportunities. In this paper, we address the above issues by designing MEMTUNE, a dynamic memory manager for in-memory data analytics. MEMTUNE dynamically tunes computation/caching memory partitions at runtime based on workload memory demand and in-memory data cache needs. Moreover, if needed, the scheduling information from the analytic framework is leveraged to evict data that will not be needed in the near future. Finally, MEMTUNE also supports task-level data prefetching with a configurable window size to more effectively overlap computation with I/O. Our experiments show that MEMTUNE improves memory utilization, yields an overall performance gain of up to 46{\%}, and achieves cache hit ratio of up to 41{\%} compared to standard Spark.},
  author = {Xu, Luna and Li, Min and Zhang, Li and Butt, Ali R. and Wang, Yandong and Hu, Zane Zhenhua},
  booktitle = {Proceedings - 2016 IEEE 30th International Parallel and Distributed Processing Symposium, IPDPS 2016},
  doi = {10.1109/IPDPS.2016.105},
  isbn = {9781509021406},
  pages = {383--392},
  pmid = {236398},
  title = {{MEMTUNE: Dynamic Memory Management for In-Memory Data Analytic Platforms}},
  year = {2016}
  }

@article{Gittens2016,
  abstract = {We explore the trade-offs of performing linear algebra using Apache Spark, compared to traditional C and MPI implementations on HPC platforms. Spark is designed for data analytics on cluster computing platforms with access to local disks and is optimized for data-parallel tasks. We examine three widely-used and important matrix factorizations: NMF (for physical plausability), PCA (for its ubiquity) and CX (for data interpretability). We apply these methods to TB-sized problems in particle physics, climate modeling and bioimaging. The data matrices are tall-and-skinny which enable the algorithms to map conveniently into Spark's data-parallel model. We perform scaling experiments on up to 1600 Cray XC40 nodes, describe the sources of slowdowns, and provide tuning guidance to obtain high performance.},
  archivePrefix = {arXiv},
  arxivId = {1607.01335},
  author = {Gittens, Alex and Devarakonda, Aditya and Racah, Evan and Ringenburg, Michael and Gerhardt, Lisa and Kottalam, Jey and Liu, Jialin and Maschhoff, Kristyn and Canon, Shane and Chhugani, Jatin and Sharma, Pramod and Yang, Jiyan and Demmel, James and Harrell, Jim and Krishnamurthy, Venkat and Mahoney, Michael W. and Prabhat},
  doi = {10.1109/BigData.2016.7840606},
  eprint = {1607.01335},
  isbn = {9781467390057},
  pages = {1--26},
  title = {{Matrix Factorization at Scale: a Comparison of Scientific Data Analytics in Spark and C+MPI Using Three Case Studies}},
  url = {http://arxiv.org/abs/1607.01335},
  year = {2016}
  }

@article{Hong2017,
  abstract = {{\textcopyright} 2017 IEEE. Due to its simplicity and scalability, MapReduce has become a de facto standard computing model for big data processing. Since the original MapReduce model was only appropriate for embarrassingly parallel batch processing, many follow-up studies have focused on improving the efficiency and performance of the model. Spark follows one of these recent trends by providing in-memory processing capability to reduce slow disk I/O for iterative computing tasks. However, the acceleration of Spark's in-memory processing using graphics processing units (GPUs) is challenging due to its deep memory hierarchy and host-To-GPU communication overhead. In this paper, we introduce a novel GPU-Accelerated MapReduce framework that extends Spark's in-memory processing so that iterative computing is performed only in the GPU memory. Having discovered that the main bottleneck in the current Spark system for GPU computing is data communication on a Java virtual machine, we propose a modification of the current Spark implementation to bypass expensive data management for iterative task offloading to GPUs. We also propose a novel GPU in-memory processing and caching framework that minimizes host-To-GPU communication via lazy evaluation and reuses GPU memory over multiple mapper executions. The proposed system employs message-passing interface (MPI)-based data synchronization for inter-worker communication so that more complicated iterative computing tasks, such as iterative numerical solvers, can be efficiently handled. We demonstrate the performance of our system in terms of several iterative computing tasks in big data processing applications, including machine learning and scientific computing. We achieved up to 50 times speed up over conventional Spark and about 10 times speed up over GPU-Accelerated Spark.},
  author = {Hong, Sumin and Choi, Woohyuk and Jeong, Won Ki},
  doi = {10.1109/CCGRID.2017.41},
  isbn = {9781509066100},
  journal = {Proceedings - 2017 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing, CCGRID 2017},
  keywords = {GPU,In-memory Computing,MapReduce,Spark},
  pages = {31--41},
  title = {{GPU in-memory processing using spark for iterative computation}},
  year = {2017}
  }

@article{Manzi2016,
  abstract = {Apache Spark is one of the most active open-source cluster computing frameworks. However, there has been very limited research on improving Spark performance using GPUs. This paper explores the feasibility and benefits of off loading Spark core operations to GPUs. Experimental results from popular Spark applications show a performance improvement of up to 17X for a K-Means Clustering application. The results are used to suggest the type of applications that will benefit the most from off loading Spark workloads to GPUs.},
  author = {Manzi, Dieudonne and Tompkins, David},
  doi = {10.1109/IC2E.2016.30},
  isbn = {9781509019618},
  journal = {Proceedings - 2016 IEEE International Conference on Cloud Engineering, IC2E 2016: Co-located with the 1st IEEE International Conference on Internet-of-Things Design and Implementation, IoTDI 2016},
  pages = {222--223},
  title = {{Exploring GPU acceleration of apache spark}},
  year = {2016}
  }

@article{BrandonHernandez2017,
  author = {{Brand{\'{o}}n Hern{\'{a}}ndez}, {\'{A}}lvaro and Perez, ıA{\'{A}}a S and Gupta, Smrati and Munt{\'{e}}s-Mulero, Victor},
  doi = {10.1016/j.future.2017.07.003},
  journal = {Future Generation Computer Systems},
  keywords = {Big data,Machine learning,Parallelism,Spark},
  title = {{Using machine learning to optimize parallelism in big data applications}},
  url = {http://dx.doi.org/10.1016/j.future.2017.07.003.},
  year = {2017}
  }

@article{Morishima,
  author = {Morishima, Shin and Okazaki, Masahiro and Matsutani, Hiroki},
  title = {{A Case for Remote GPUs over 10GbE Network for VR Applications}},
  year = {2017}
  }

@article{Nicolae2016,
  abstract = {Big data analytics is an indispensable tool in transforming science,
  engineering, medicine, healthcare, finance and ultimately business
  itself. With the explosion of data sizes and need for shorter
  time-to-solution, in-memory platforms such as Apache Spark gain
  increasing popularity. However, this introduces important challenges,
  among which data shuffling is particularly difficult: on one hand it is
  a key part of the computation that has a major impact on the overall
  performance and scalability so its efficiency is paramount, while on the
  other hand it needs to operate with scarce memory in order to leave as
  much memory available for data caching. In this context, efficient
  scheduling of data transfers such that it addresses both dimensions of
  the problem simultaneously is non-trivial. State-of-the-art solutions
  often rely on simple approaches that yield sub-optimal performance and
  resource usage. This paper contributes a novel shuffle data transfer
  strategy that dynamically adapts to the computation with minimal memory
  utilization, which we briefly underline as a series of design
  principles.},
  author = {Nicolae, Bogdan and Costa, Carlos and Misale, Claudia and Katrinis, Kostas and Park, Yoonho},
  doi = {10.1109/CCGrid.2016.85},
  isbn = {9781509024520},
  issn = {2376-4414},
  journal = {Proceedings - 2016 16th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing, CCGrid 2016},
  keywords = {Elastic buffering,big data analytics,data shuffling,memory-efficient I/O},
  pages = {409--412},
  pmid = {174955},
  title = {{Towards Memory-Optimized Data Shuffling Patterns for Big Data Analytics}},
  year = {2016}
  }

@article{Grossman2016,
  abstract = {The field of data analytics is currently going through a renaissance as a result of ever-increasing dataset sizes, the value of the models that can be trained from those datasets, and a surge in exible, distributed programming models. In particular, the Apache Hadoop [1] and Spark [5] programming systems, as well as their supporting projects (e.g. HDFS, SparkSQL), have greatly simplified the analysis and transformation of datasets whose size exceeds the capacity of a single machine. While these programming models facilitate the use of distributed systems to analyze large datasets, they have been plagued by performance issues. The I/O performance bottlenecks of Hadoop are partially responsible for the creation of Spark. Performance bottlenecks in Spark due to the JVM object model, garbage collection, interpreted/-managed execution, and other abstraction layers are responsible for the creation of additional optimization layers, such as Project Tungsten [4]. Indeed, the Project Tungsten issue tracker states that the "majority of Spark workloads are not bottlenecked by I/O or network, but rather CPU and memory" [20]. In this work, we address the CPU and memory performance bottlenecks that exist in Apache Spark by accelerating user-written computational kernels using accelerators. We refer to our approach as Spark With Accelerated Tasks (SWAT). SWAT is an accelerated data analytics (ADA) framework that enables programmers to natively execute Spark applications on high performance hardware platforms with co-processors, while continuing to write their applications in a JVM-based language like Java or Scala. Runtime code generation creates OpenCL kernels from JVM bytecode, which are then executed on OpenCL accelerators. In our work we emphasize 1) full compatibility with a modern, existing, and accepted data analytics platform, 2) an asynchronous, event-driven, and resource-aware runtime, 3) multi-GPU memory management and caching, and 4) ease-of-use and programmability. Our performance evaluation demonstrates up to 3.24{\&}times; overall application speedup relative to Spark across six machine learning benchmarks, with a detailed investigation of these performance improvements. Copyright {\&}copy; 2016 by the Association for Computing Machinery, Inc. (ACM).},
  author = {Grossman, Max and Sarkar, Vivek},
  doi = {10.1145/2907294.2907307},
  isbn = {9781450343145},
  journal = {HPDC 2016 - Proceedings of the 25th ACM International Symposium on High-Performance Parallel and Distributed Computing},
  keywords = {Abstracting;Ada (programming language);Artificial},
  pages = {81--92},
  pmid = {209269},
  title = {{SWAT: A programmable, in-memory, distributed, high-performance computing platform}},
  url = {http://dx.doi.org/10.1145/2907294.2907307},
  year = {2016}
  }

@article{Anderson2017,
  abstract = {Apache Spark is a popular framework for data analytics with attractive features such as fault tolerance and interoperabil-ity with the Hadoop ecosystem. Unfortunately, many an-alytics operations in Spark are an order of magnitude or more slower compared to native implementations written with high performance computing tools such as MPI. There is a need to bridge the performance gap while retaining the benefits of the Spark ecosystem such as availability, pro-ductivity, and fault tolerance. In this paper, we propose a system for integrating MPI with Spark and analyze the costs and benefits of doing so for four distributed graph and ma-chine learning applications. We show that offloading compu-tation to an MPI environment from within Spark provides 3.1−17.7× speedups on the four sparse applications, includ-ing all of the overheads. This opens up an avenue to reuse existing MPI libraries in Spark with little effort.},
  author = {Anderson, Michael and Smith, Shaden and Sundaram, Narayanan and Capotă, Mihai and Zhao, Zheguang and Dulloor, Subramanya and Satish, Nadathur and Willke, Theodore L.},
  doi = {10.14778/3090163.3090168},
  issn = {21508097},
  journal = {Proceedings of the VLDB Endowment},
  number = {8},
  pages = {901--912},
  title = {{Bridging the gap between HPC and big data frameworks}},
  url = {http://dl.acm.org/citation.cfm?doid=3090163.3090168},
  volume = {10},
  year = {2017}
  }

@article{Segal2015,
  abstract = {We introduce SparkCL, an open source unified programming framework based on Java, OpenCL and the Apache Spark framework. The motivation behind this work is to bring unconventional compute cores such as FPGAs/GPUs/APUs/DSPs and future core types into mainstream programming use. The framework allows equal treatment of different computing devices under the Spark framework and introduces the ability to offload computations to acceleration devices. The new framework is seamlessly integrated into the standard Spark framework via a Java-OpenCL device programming layer which is based on Aparapi and a Spark programming layer that includes new kernel function types and modified Spark transformations and actions. The framework allows a single code base to target any type of compute core that supports OpenCL and easy integration of new core types into a Spark cluster.},
  archivePrefix = {arXiv},
  arxivId = {1505.01120},
  author = {Segal, O and Colangelo, P and Nasiri, N},
  eprint = {1505.01120},
  journal = {arXiv preprint arXiv: {\ldots}},
  title = {{SparkCL: A Unified Programming Framework for Accelerators on Heterogeneous Clusters}},
  url = {http://arxiv.org/abs/1505.01120},
  year = {2015}
  }

@article{Hassaan2016,
  author = {Hassaan, Mohamed and Elghandour, Iman},
  isbn = {9781450346177},
  keywords = {GPU Programming,Heterogeneous clusters,In-memory cluster computing,gpu programming,heterogeneous clusters,in-memory clus-},
  pages = {168--177},
  title = {{A Real-Time Big Data Analysis Framework on a CPU / GPU Heterogeneous Cluster}},
  year = {2016}
  }

@article{Liang2015,
  abstract = {Current popular systems, Hadoop and Spark, cannot achieve satisfied performance because of the inefficient overlapping of computation and communication when running iterative big data applications. The pipeline of computing, data movement, and data management plays a key role for current distributed data computing systems. In this paper, we first analyze the overhead of shuffle operation in Hadoop and Spark when running PageRank workload, and then propose an event-driven pipeline and in-memory shuffle design with better overlapping of computation and communication as DataMPI-Iteration, an MPI-based library, for iterative big data computing. Our performance evaluation shows DataMPI-Iteration can achieve 9X∼21X speedup over Apache Hadoop, and 2X∼3X speedup over Apache Spark for PageRank and K-means. {\textcopyright} 2015, Springer Science+Business Media New York.},
  author = {Liang, Fan and Lu, Xiaoyi},
  doi = {10.1007/s11390-015-1522-5},
  isbn = {1139001515},
  issn = {10009000},
  journal = {Journal of Computer Science and Technology},
  keywords = {DataMPI,Hadoop MapReduce,Spark,iterative computation},
  number = {2},
  pages = {283--294},
  title = {{Accelerating Iterative Big Data Computing Through MPI}},
  volume = {30},
  year = {2015}
  }

@article{Ohno2017,
  abstract = {Apache Spark is a distributed processing framework for large-scale data sets, where intermediate data sets are represented as RDDs (Resilient Distributed Datasets) and stored in memory distributed over machines. To accelerate its various computation intensive operations, such as reduction and sort, we focus on GPU devices. We modified Spark framework to invoke CUDA kernels when computation intensive operations are called. RDDs are transformed into array structures and transferred to GPU devices when necessary. Although we need to cache RDDs in GPU device memory as much as possible in order to hide the data transfer overhead, the number of local GPU devices mounted in a host machine is limited. In this paper, we propose to use remote GPU devices which are connected to a host machine via a PCI-Express over 10Gbps Ethernet technology. To mitigate the data transfer overhead for remote GPU devices, we propose three RDD caching policies for local and remote GPU devices. We implemented various reduction programs (e.g., Sum, Max, LineCount) and transformation programs (e.g., SortByKey, PatternMatch, WordConversion) using local and remote GPU devices for Spark. Evaluation results show that Spark with GPU outperforms the original software by up to 21.4x. We also evaluate the RDD caching policies for local and remote GPU devices and show that a caching policy that minimizes the data transfer amount for remote GPU devices achieves the best performance.},
  author = {Ohno, Yasuhiro and Morishima, Shin and Matsutani, Hiroki},
  doi = {10.1109/ICPADS.2016.0108},
  isbn = {9781509044573},
  issn = {15219097},
  journal = {Proceedings of the International Conference on Parallel and Distributed Systems - ICPADS},
  keywords = {Apache Spark,CUDA,GPU,PCIe over 10GbE,RDD},
  pages = {791--799},
  title = {{Accelerating spark RDD operations with local and remote GPU devices}},
  year = {2017}
  }
