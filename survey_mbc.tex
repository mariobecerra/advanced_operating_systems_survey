\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{blindtext}
\usepackage{graphicx}
\usepackage[pass]{geometry}
\usepackage[backend=bibtex]{biblatex}

\addbibresource{bibliography.bib} %Imports bibliography file

\begin{document}

\newgeometry{bottom=2.5cm, left=2.1cm,right=2.1cm,top=2.5cm}

\title{Survey name (something about improving Apache Spark)}

\author{Mario~Becerra Contreras}

\date{Fall 2017}


\maketitle

\begin{abstract}

We got a pretty nice abstract regarding the subject that is being studied and analyzed. This is a superb abstract, like, really really good. It's like the best abstract ever, for real. This is the best abstract you'll see in quite a long time.

\end{abstract}

\section{Introduction}

Very cool intro.


\section{Overview of Apache Spark}

Apache Spark \cite{zaharia_spark:_2010} is an open-source cluster computing framework that started in 2009 at the University of California, Berkeley \cite{zaharia_apache_2016}. The project aimed at creating a unified engine for distributed data processing by using a model similar to MapReduce but with data-sharing capabilities. This data-sharing was implemented with an abstraction called Resilient Distributed Datasets (or RDDs) \cite{zaharia_resilient_2012}. The idea was that in traditional MapReduce jobs, iterative jobs as the ones that surge naturally in machine learning, graph algorithms and interactive data mining suffer from a high overhead because of the continuous reading of the same data from disk. This could be improved by leveraging distributed memory. Back in the day, the only way to reuse data between computations was to write in a file system, which also incurred in a lot of overhead. 

Today, Spark is commonly used with distributed file systems such as Hadoop Distributed File System (HDFS) \cite{Shvachko2010}. It has grown and become one of the most active open-source projects, having over 1000 contributors and used in more than 1000 companies \cite{zaharia_apache_2016}. It is used widely in industry and has more than 350 third-party packages \cite{SparkPackages}, and four main high-level libraries that facilitate the use of Spark: SparkSQL for relational queries \cite{armbrust2015spark}, Spark Streaming for discretized streams \cite{zaharia2013discretized}, GraphX for graph computations \cite{gonzalez2014graphx}, and MLlib for machine learning \cite{Meng2016}. These four libraries can be easily combined in applications.

An RDD is a fault-tolerant representation of a read-only collection of partitioned objects across many machines. RDDs can be created through two operations: data from a file system or from other RDDs. These operations are called transformations, and examples include \textit{map}, \textit{filter} and \textit{join}. These operations are evaluated lazily, this way an efficient plan to compute them can be executed. The evaluated is executed until an \textit{action} (like \textit{count}, \textit{collect} or \textit{save} operations) is called. RDDs are reliable because they can be efficiently rebuilt in case a partition is lost. They do this by keeping a log of the graph of transformations that were used to build an RDD (this is called the \textit{lineage} of the RDD). Recovery is faster than rerunning the program because a failed has several partitions which can be rebuilt on other nodes in parallel.

There are two types of dependencies in RDDs: narrow and wide. In wide narrow dependencies, each partition of the parent RDD is used by at most one partition of the child RDD; whilst in wide dependencies multiple partitions may depend on the same partition of the parent RDD. Wide dependencies are communication intensive because they involve data shuffle across the network and thus may be a performance bottleneck.

By default, RDDs are recomputed every time an action is performed, but users have the option to tell Spark to keep an RDD in memory. If the RDD doesn't fit in memory, it is spilled to disk. This option is what can make Spark several orders of magnitude faster than plain MapReduce jobs on Hadoop in iterative problems. Since the RDDs can be kept in memory, the reused dataset isn't loaded in every iteration. RDDs can express several cluster programming models in an efficient way; models such as MapReduce, SQL, Interactive MapReduce and Batched Stream Processing \cite{zaharia_resilient_2012}.




\section{Survey}

Even though Spark is a powerful computing framework, it is not perfect and has its caveats. There's been a handful of efforts to improve different areas of opportunity. Some are general improvements, whilst other are particularities that are suited for some tasks.

\subsection{Improvement based on communication}

As mentioned before, there's a performance bottleneck in the shuffling phase of wide dependencies. It is common to compress files before sending them in the network to alleviate this, but this is too dependent on the structure of the input data. \citeauthor{davidson2013optimizing} \cite{davidson2013optimizing} propose some alternatives to solve the problems that stress the OS during the shuffle phase. Since the number of shuffle files may be in the order of the millions in a usual Spark work, the number of random writes is quite big. The main proposed solution to this problem is to create larger shuffle files. The results show that the I/O wait time is lower with this modification, but disk throughput is sustained, meaning that much less random I/O is done by creating fewer shuffle files. They suggest as future work to do an in-memory shuffle when it is possible to fit the data in the cluster's collective memory.

This issue was addressed some time later by the Spark group themselves. \citeauthor{Armbrust2015} \cite{Armbrust2015} recognize the problem of shuffling operations on many nodes, along with some other memory and usability issues. The former affair was resolved by changing the original NIO-based network module with a new implementation based on Netty. The latter problems were specifically that memory management needed to be improved, that performance debugging was hard to do, and that there was some confusion about Spark's API.

The memory management issue was taken care of adding more caps to take care of compressed data that could scale from a few hundreds of MBs to a couple of GBs. As for the debugging, new metrics were added to the monitoring UI; allowing users to see things such as time taken to schedule, run and receive results from tasks. They also added new data visualization tools to monitor the jobs. The confusion about the API was addressed by developing a more declarative API, one that is based in data frames, such as the ones in Python and R. With this new API, the standard data frame interface is compiled using the optimizer in SparkSQL \cite{armbrust2015spark}. These data frames are also now being used as input and output between Spark's high level libraries. These changes were introduced in Spark's version 1.2.0.

One early work was to adapt RDMA (Remote Direct Memory Access) to Spark in an InfiniBand Architecture \cite{Lu2014}. The InfiniBand Architecture is ``a new industry-standard architecture for server I/O and inter-server communication'' \cite{pfister2001introduction}. The authors claim that Spark can't fully use InfiniBand's capabilities to obtain optimal performance, so they adapt RDMA to be a part of the communication infrastructure by overriding Spark's native features so RDMA calls would be used instead of the Java Socket interface. Their adaptation achieved an 83\% performance improvement in the test tasks that they used and conclude that with their RDMA-base design Spark applications could be improved.

This same group continued with this study \cite{Lu2016}: in \citeyear{Lu2016} they tested an advanced RDMA-based design in the changed Apache architecture that came with version 1.2.0 \cite{Armbrust2015}. They particularly check for improvements based on RDMA in the new shuffle architecture based in Netty instead of NIO. They implement an advanced design in three clusters with the most recent InfiniBand technologies and test several workloads on RDD Benchmarks, graph processing and SQL queries on top of Spark scaling up to 1536 cores. The results show that they achieve up to 79\% improvement in RDD performance, 46\% for graph workloads and 32\% for SQL queries. 


\subsection{Improvement based on file system, OS and JVM parameters}

Another work \cite{Islam2015} focused on studying the impacts of two file systems (Tachyon and Triple-H) on Spark applications. In particular, they identified critical parameters on the performance of the applications that run on top of these file systems, such as BlockSize, concurrent containers and concurrent tasks. They also propose selective caching into high performance memory layers (such as RAMDisks or SSDs). This selective caching refers to caching the data that is the output of an iteration coming from a map or reduce task. One more optimization comes from improving the cache eviction algorithm after each algorithm iteration. They see that CPU-bound workloads don't show as much benefit as I/O intensive ones, and that iterative applications gain performance over the Triple-H file system.

\citeauthor{Wang2016} \cite{Wang2016} study the behaviour of Linux virtual memory to find out the effect of kernel parameters. They test this with Spark workloads that are especially sensitive to memory parameter tuning. They test the effect of transparent huge page but find none. They find that enabling Non-uniform memory access (NUMA) yields better results for some workloads, though not all. The conclusion of this work is that better results can be achieved by tuning certain parameters, although this study was small and not so generalizable.

Since Spark runs on top of Java Virtual Machines (JVMs), another way to improve performance is to tune JVM parameters. \citeauthor{Chiba2016} use different JVM and OS parameters to try to take full advantage of computing resources \cite{Chiba2016}. They argue that Spark creates many objects in heaps, so garbage collection (GC) pause time is a parameter that may cause execution time to be too long. They propose a series of optimizations (such as reducing the nursery space in the heap, changing the number of JVMs in a single node and applying NUMA affinity) and then test them in the TPC-H query benchmark,  which is ``a decision support benchmark [that] consists of a suite of business oriented ad-hoc queries and concurrent data modifications'' \cite{TPC_H_benchmark}. They find that these optimizations performed up to 5 time faster and ran 30\% to 40\% faster on average.
% These guys cite some other papers that do similar work as theirs. Check them out. They also mention some future work on other Spark workloads. See if they continued.

Another similar work was done by \citeauthor{Hema2016}, testing the behaviour of Spark under different JVM parameters (heap size and GC frequency), testing it with machine learning case studies, particularly k-means, matrix factorization and logistic regression \cite{Hema2016}. They found that k-means and logistic regression didn't have performance issues in their tests because heap size was sufficient and it didn't need much garbage collection, whilst matrix factorization had a heap memory bottleneck because of the large number of created objects, and consequently GC is called rather frequently. They say that performance can be improved by increasing the executor memory.

\subsection{GPU-based improvement}

Some different work was done in \cite{Li2015}, where the main contribution was related to GPU computing. They propose ``the marriage of GPUs to CPU-based cluster computing framework''. To do so, they implement HeteroSpark, a framework that augments the usual Spark framework by using GPUs in Spark worker nodes. The function calls from Spark are read by HeteroSpark and directed to the GPU via Java's Remote Method Invocation (RMI). If a function call isn't implemented in the GPU, the developer may opt to use the original implementation or decide to implement it themselves. The results show that, not surprisingly, performance is much better with Heterospark. This is because of the GPU acceleration. What the authors remark is that this improvement was achieved without sacrificing neither programmability nor portability.

Another effort was made by \citeauthor{Manzi2016}, who explore the feasibility and potential benefits of combining GPUs and Spark. Since most non-shuffling operations can be completed on GPUs, their implementation focuses on porting these operations to the GPUs \cite{Manzi2016}. Their implementation first has to convert original data to a GPU-compatible format, and their conclusion from the analysis is that ``the performance improvement for any application will depend on how much the gain from GPU thread-level parallelism overshadows the data conversion cost'', e.g., in an implementation for WordCount, the CPU converts from unicode characters to ASCII, and this step takes 57\% of the total time \cite{Manzi2016}. In the K-means algorithm, data shuffling is a minor part of the process, so the gains are much higher; so it is better to use GPUs with algorithms that make many iterations over the same dataset so that the data preparation cost is offset.

\citeauthor{Hassaan2016} introduce DAMB, a system for streaming data in a heterogeneous cluster with GPUs and CPUs, and SparkGPU, its core Spark extension that manages and delegates task in the cluster \cite{Hassaan2016}. Their work is tested in a real-time lightning monitoring station that generates 1.6 GB of data per second and achieve very good results. 

\citeauthor{Ohno2017} accelerate RDD operations for a single computer using GPUs that are either mounted locally or remotely, leveraging how many partitions of RDDs are cached in local memory and how many in remote memory \cite{Ohno2017}. Their implementation shows a speed-up of up to 21 times compared to Vanilla Spark.

Despite these efforts, caching data in GPU memory for iterative processes had not yet been fully explored, until the work of \citeauthor{Hong2017} came along \cite{Hong2017}. They noted that in most of the GPU-accelerated applications came from the system overhead and not from the computations themselves. They present a PySpark based extension for Spark that does efficient GPU in-memory caching and processing. They show that their system improves the performance of different iterative algorithms, such as logistic regression and k-means.


\subsection{Performance prediction based improvement}

\citeauthor{wang2015performance} \cite{wang2015performance} present a model that predicts execution time, memory consumption and I/O cost. The model first executes the Spark program on a cluster using a sample of the data, thus collecting information about execution time, memory consumption and I/O costs. They find that their predictions are accurate for execution time and memory consumption, but not so for I/O cost. They conclude that this approach can help better allocate resources in a cluster running Spark.

In \citeyear{Ulanov2017}, \citeauthor{Ulanov2017} develop a model to that captures the processing time of a given machine learning algorithm in a parallelized computing cluster \cite{Ulanov2017}. Their study aims at modeling the scalability of an algorithm so that the modeler can have an idea in advance of how much the task will take in a given cluster architecture.

In a similar line of improvement, \citeauthor{Marco2017} \cite{Marco2017} develop a machine learning algorithm that predicts the memory requirement of any Spark application in order to use effective task co-location strategies. The machine learning model is trained for each specific task by taking a subset of the dataset that is going to be used and then fitting a set of linear and non-linear regressions. Their approach was tested with a range of Spark applications that cover different application domains and show an accurate system modeling model. Because of these accurate predictions, the memory resources are better utilized and thus the system achieves performance improvements by effectively using the runtime task scheduler.

\subsection{Improvements for HPC systems}

Even though Spark is most commonly used in large data centres, it can also be use in an HPC context, with supercomputers, such as Cray supercomputers. \citeauthor{Chaimov2016} study the performance of Spark in two Cray XC systems in a large supercomputing centre \cite{Chaimov2016}. They first test the performance of a single workstation with SSDs with a single node of a Cray XC and found that the Cray node was slower. They attributed this to the latency involved in opening files (in \texttt{fopen}). To calibrate the performance in the Cray they propose using a local file system in a Lustre file or main memory and to use pooling when opening files so that metadata is cached.
%They argue that in supercomputer settings, disk I/O is optimized for bandwidth and network is optimized for latency, unlike in data centres where disk I/O is primarily optimized for latency and network is optimized for bandwidth.

HPC clusters are increasingly being equipped with fast high-performance storage, such as SDDs and RAMDisks, and with high-performance interconnects; although not all nodes in the same HPC cluster share the same characteristics: it is common to have SDDs in some nodes and normal HDDs in others. Since the Spark scheduler assumes homogeneity in the nodes, it schedules taking into account the locality of the data, which may not be the best idea for a heterogeneous system \cite{Islam2016}. \citeauthor{Islam2016} consider some approaches to solve the challenge of choosing a criteria for efficient data access and redesign the HDFS to include their strategies, leading to performance improvement compared to the locality based data access in the benchmarks tested \cite{Islam2016}.

Another recent effort that has been made is to combine MPI with Spark. \citeauthor{Morris2017} present MPIgnite \cite{Morris2017}, a framework that introduces peer-to-peer communication concepts into Spark. And the most recent work in the area of improving spark for HPC systems was made by \citeauthor{kim2017sparkle}. In their article they introduce Sparkle \cite{kim2017sparkle}, a library that enhances Spark for scale-up servers, such as HPC systems, leveraging the shared memory these systems have. With shared memory, data shuffling is less inefficient because there's no dependence on data transfer between distant nodes. Vanilla Spark makes the JVM garbage collector work on deleting a lot of objects in the heap memory of the worker nodes, but Sparkle takes care of this by efficiently storing data in the global shared memory. In the performance tests, Sparkle in a scale-up system showed an improvement of 1.6 up to 5 times in comparison to Vanilla Spark running on a scale-out cluster with equivalent hardware, this mainly due to the faster communication between the nodes.




\section{Conclusion and future work}

Very nice conclusions.


\printbibliography
%\nocite{*}


\end{document}
