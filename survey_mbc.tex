\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{blindtext}
\usepackage{graphicx}
\usepackage[pass]{geometry}
\usepackage[backend=bibtex]{biblatex}

\addbibresource{bibliography.bib} %Imports bibliography file

\begin{document}

\newgeometry{bottom=2.5cm, left=2.1cm,right=2.1cm,top=2.5cm}

\title{Survey name (something about improving Apache Spark)}

\author{Mario~Becerra Contreras}

\date{Fall 2017}


\maketitle

\begin{abstract}

We got a pretty nice abstract regarding the subject that is being studied and analyzed. This is a superb abstract, like, really really good. It's like the best abstract ever, for real. This is the best abstract you'll see in quite a long time.

\end{abstract}

\section{Introduction}

Very cool intro.


\section{Overview of Apache Spark}

Apache Spark \cite{zaharia_spark:_2010} is an open-source cluster computing framework that started in 2009 at the University of California, Berkeley \cite{zaharia_apache_2016}. The project aimed at creating a unified engine for distributed data processing by using a model similar to MapReduce but with data-sharing capabilities. This data-sharing was implemented with an abstraction called Resilient Distributed Datasets (or RDDs) \cite{zaharia_resilient_2012}. The idea was that in traditional MapReduce jobs, iterative jobs as the ones that surge naturally in machine learning, graph algorithms and interactive data mining suffer from a high overhead because of the continuous reading of the same data from disk. This could be improved by leveraging distributed memory. Back in the day, the only way to reuse data between computations was to write in a file system, which also incurred in a lot of overhead. 

Today, Spark is commonly used with distributed file systems such as Hadoop Distributed File System (HDFS) \cite{Shvachko2010}. It has grown and become one of the most active open-source projects, having over 1000 contributors and used in more than 1000 companies \cite{zaharia_apache_2016}. It is used widely in industry and has more than 350 third-party packages \cite{SparkPackages}, and four main high-level libraries that facilitate the use of Spark: SparkSQL for relational queries \cite{Armbrust}, Spark Streaming for discretized streams \cite{Zahariab}, GraphX for graph computations \cite{Gonzalez}, and MLlib for machine learning \cite{Meng2016}. These four libraries can be easily combined in applications.

An RDD is a fault-tolerant representation of a read-only collection of partitioned objects across many machines. RDDs can be created through two operations: data from a file system or from other RDDs. These operations are called transformations, and examples include \textit{map}, \textit{filter} and \textit{join}. These operations are evaluated lazily, this way an efficient plan to compute them can be executed. The evaluated is executed until an \textit{action} (like \textit{count}, \textit{collect} or \textit{save} operations) is called. RDDs are reliable because they can be efficiently rebuilt in case a partition is lost. They do this by keeping a log of the graph of transformations that were used to build an RDD (this is called the \textit{lineage} of the RDD). Recovery is faster than rerunning the program because a failed has several partitions which can be rebuilt on other nodes in parallel.

There are two types of dependencies in RDDs: narrow and wide. In wide narrow dependencies, each partition of the parent RDD is used by at most one partition of the child RDD; whilst in wide dependencies multiple partitions may depend on the same partition of the parent RDD. Wide dependencies are communication intensive because they involve data shuffle across the network and thus may be a performance bottleneck.

By default, RDDs are recomputed every time an action is performed, but users have the option to tell Spark to keep an RDD in memory. If the RDD doesn't fit in memory, it is spilled to disk. This option is what can make Spark several orders of magnitude faster than plain MapReduce jobs on Hadoop in iterative problems. Since the RDDs can be kept in memory, the reused dataset isn't loaded in every iteration. RDDs can express several cluster programming models in an efficient way; models such as MapReduce, SQL, Interactive MapReduce and Batched Stream Processing \cite{zaharia_resilient_2012}.




\section{Survey}

Even though Spark is a powerful computing framework, it is not perfect and has its caveats. There's been a handful of efforts to improve different areas of opportunity. Some are general improvements, whilst other are particularities that are suited for some tasks.

One early work was to adapt RDMA (Remote Direct Memory Access) to Spark in an InfiniBand Architecture \cite{Lu2014}. The InfiniBand Architecture is ``a new industry-standard architecture for server I/O and inter-server communication'' \cite{pfister2001introduction}. The authors claim that Spark can't fully use InfiniBand's capabilities to obtain optimal performance, so they adapt RDMA to be a part of the communication infrastructure by overriding Spark's native features so RDMA calls would be used instead of the Java Socket interface. Their adaptation achieved an 83\% performance improvement in the test tasks that they used and conclude that with their RDMA-base design Spark applications could be improved.

As mentioned before, there's a performance bottleneck in the shuffling phase of wide dependencies. It is common to compress files before sending them in the network to alleviate this, but this is too dependent on the structure of the input data. \citeauthor{Davidson} \cite{Davidson} propose some alternatives to solve the problems that stress the OS during the shuffle phase. Since the number of shuffle files may be in the order of the millions in a usual Spark work, the number of random writes is quite big. The main proposed solution to this problem is to create larger shuffle files. The results show that the I/O wait time is lower with this modification, but disk throughput is sustained, meaning that much less random I/O is done by creating fewer shuffle files. They suggest as future work to do an in-memory shuffle when it is possible to fit the data in the cluster's collective memory.

Another work \cite{Islam2015} focused on studying the impacts of two file systems (Tachyon and Triple-H) on Spark applications. In particular, they identified critical parameters on the performance of the applications that run on top of these file systems, such as BlockSize, concurrent containers and concurrent tasks. They also propose selective caching into high performance memory layers (such as RAMDisks or SSDs). This selective caching refers to caching the data that is the output of an iteration coming from a map or reduce task. One more optimization comes from improving the cache eviction algorithm after each algorithm iteration. They see that CPU-bound workloads don't show as much benefit as I/O intensive ones, and that iterative applications gain performance over the Triple-H file system.

\citeauthor{Wang2016} \cite{Wang2016} study the behaviour of Linux virtual memory to find out the effect of kernel parameters. They test this with Spark workloads that are especially sensitive to memory parameter tuning. They test the effect of transparent huge page but find none. They find that enabling Non-uniform memory access (NUMA) yields better results for some workloads, though not all. The conclusion of this work is that better results can be achieved by tuning certain parameters, although this study was small and not so generalizable.

Some different work was done in \cite{Li2015}, where the main contribution was related to GPU computing. They propose ``the marriage of GPUs to CPU-based cluster computing framework''. To do so, they implement HeteroSpark, a framework that augments the usual Spark framework by using GPUs in Spark worker nodes. The function calls from Spark are read by HeteroSpark and directed to the GPU via Java's Remote Method Invocation (RMI). If a function call isn't implemented in the GPU, the developer may opt to use the original implementation or decide to implement it themselves. The results show that, not surprisingly, performance is much better with Heterospark. This is because of the GPU acceleration. What the authors remark is that this improvement was achieved without sacrificing programmability nor portability.

In another line of improvement, \citeauthor{Marco2017} \cite{Marco2017} develop a machine learning algorithm that predicts the memory requirement of any Spark application in order to use effective task co-location strategies. The machine learning model is trained for each specific task by taking a subset of the dataset that is going to be used and then fitting a set of linear and non-linear regressions. Their approach was tested with a range of Spark applications that cover different application domains and show an accurate system modeling model. Because of these accurate predictions, the memory resources are better utilized and thus the systems achieves performance improvements by effectively using the runtime task scheduler.


\section{Conclusion and future work}

Very nice conclusions.


\printbibliography
%\nocite{*}


\end{document}
