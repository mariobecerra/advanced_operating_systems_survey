\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{blindtext}
\usepackage{graphicx}
\usepackage[pass]{geometry}
\usepackage[backend=bibtex, maxbibnames=99]{biblatex}


\addbibresource{bibliography.bib} %Imports bibliography file

\begin{document}

\newgeometry{bottom=2.5cm, left=2.1cm,right=2.1cm,top=2.5cm}

\title{On improving Apache Spark's performance}

\author{Mario~Becerra Contreras}

\date{Fall 2017}


\maketitle

\begin{abstract}

We got a pretty nice abstract regarding the subject that is being studied and analyzed. This is a superb abstract, like, really really good. It's like the best abstract ever, for real. This is the best abstract you'll see in quite a long time.

\end{abstract}

\section{Introduction}

In the last years there has been a growing amount of information in industry and research, mainly due to the internet and improvement of computers, which has given rise to different areas of research that did not exist before. One of these areas deals with the storing of massive amounts of information, and another one has to do with the processing of this information. MapReduce \cite{dean2008mapreduce} is one of the most famous programming models that deals with large data sets on large clusters of commodity machines. Then Hadoop was introduced along with the Hadoop Distributed File System (HDFS) \cite{Shvachko2010}, allowing the analysis of large datasets, all using the MapReduce paradigm.

While the HDFS and its underlying technologies provided good performance for using large datasets, they were inefficient for a specific but quite used type of applications: applications that reuse a working set of data. Reusing of data is very common in machine learning applications where it is expected to minimize an loss function, so most algorithms iteratively optimize parameters using techniques such as gradient descent or other numerical optimization methods. Because of this need to have an efficient framework to do iterative algorithms in the same data, Apache Spark was introduced \cite{zaharia_resilient_2012} \cite{zaharia_spark:_2010}.

In this work, it is intended to give a short review of the optimizations and improvements that have been made on Apache Spark over the years since its introduction. 

The rest of the work is organized as follows: in section \ref{sec:spark_overview} an overview of Apache Spark is given, so that the reader has some context; section \ref{sec:survey} surveys and summarizes different published papers on the subject of the improvement of Spark; and section \ref{sec:conclusion} discusses the conclusions of the work and the direction of future work and improvement in this area.


\section{Overview of Apache Spark}
\label{sec:spark_overview}

Apache Spark \cite{zaharia_spark:_2010} is an open-source cluster computing framework that started in 2009 at the University of California, Berkeley. The project aimed at creating a unified engine for distributed data processing by using a model similar to MapReduce but with data-sharing capabilities. This data-sharing was implemented with an abstraction called Resilient Distributed Datasets (or RDDs) \cite{zaharia_resilient_2012}. The idea was that in traditional MapReduce jobs, iterative jobs as the ones that surge naturally in machine learning, graph algorithms and interactive data mining suffer from a high overhead because of the continuous reading of the same data from disk. This could be improved by leveraging distributed memory. Back in the day, the only way to reuse data between computations was to write in a file system, which also incurred in a lot of overhead. 

Today, Spark is commonly used with distributed file systems such as Hadoop Distributed File System (HDFS) \cite{Shvachko2010}. It has grown and become one of the most active open-source projects, having over 1000 contributors and used in more than 1000 companies \cite{zaharia_apache_2016}. It is used widely in industry and has more than 350 third-party packages \cite{SparkPackages}, and four main high-level libraries that facilitate the use of Spark: SparkSQL for relational queries \cite{armbrust2015spark}, Spark Streaming for discretized streams \cite{zaharia2013discretized}, GraphX for graph computations \cite{gonzalez2014graphx}, and MLlib for machine learning \cite{Meng2016}. These four libraries can be easily combined in applications.

An RDD is a fault-tolerant representation of a read-only collection of partitioned objects across many machines. RDDs can be created through two operations: data from a file system or from other RDDs. These operations are called transformations, and examples include \textit{map}, \textit{filter} and \textit{join}. These operations are evaluated lazily, this way an efficient plan to compute them can be executed. The evaluated is executed until an \textit{action} (like \textit{count}, \textit{collect} or \textit{save} operations) is called. RDDs are reliable because they can be efficiently rebuilt in case a partition is lost. They do this by keeping a log of the graph of transformations that were used to build an RDD (this is called the \textit{lineage} of the RDD). Recovery is faster than rerunning the program because a failed has several partitions which can be rebuilt on other nodes in parallel.

There are two types of dependencies in RDDs: narrow and wide. In narrow dependencies, each partition of the parent RDD is used by at most one partition of the child RDD; whilst in wide dependencies multiple partitions may depend on the same partition of the parent RDD. Wide dependencies are communication intensive because they involve data shuffle across the network and thus may be a performance bottleneck.

By default, RDDs are recomputed every time an action is performed, but users have the option to tell Spark to keep an RDD in memory. If the RDD does not fit in memory, it is spilled to disk. This option is what can make Spark several orders of magnitude faster than plain MapReduce jobs on Hadoop in iterative problems. Since the RDDs can be kept in memory, the reused dataset is not loaded in every iteration. RDDs can express several cluster programming models in an efficient way; models such as MapReduce, SQL, Interactive MapReduce and Batched Stream Processing \cite{zaharia_resilient_2012}.



\section{Survey}
\label{sec:survey}

Even though Spark is a powerful computing framework, it is not perfect and has its caveats. There has been a handful of efforts to improve different areas of opportunity. Some are general improvements, whilst other are particularities that are suited for some tasks. For this work, the improvements are divided in five rather broad and somewhat overlapping areas: based on communication; based on file system, OS and JVM parameters; based on GPUs; based on performance prediction; and for HPC systems.

\subsection{Improvement based on communication}

As mentioned before, there is a performance bottleneck in the shuffling phase of wide dependencies. It is common to compress files before sending them in the network to alleviate this, but this is too dependent on the structure of the input data. Another problem with Spark is that it originally did not fully use the potential of cluster with high-speed connections. There has been work done to explicitly address these issues, and some of the efforts are shown in the following paragraphs.

\citeauthor{davidson2013optimizing} \cite{davidson2013optimizing} propose some alternatives to solve the problems that stress the OS during the shuffle phase. Since the number of shuffle files may be in the order of the millions in a usual Spark work, the number of random writes is quite big. The main proposed solution to this problem is to create larger shuffle files. The results show that the I/O wait time is lower with this modification, but disk throughput is sustained, meaning that much less random I/O is done by creating fewer shuffle files. They suggest as future work to do an in-memory shuffle when it is possible to fit the data in the cluster's collective memory.

\citeauthor{Liang2015} tried to improve the shuffling phase with an event-driven MPI-based communication design called DataMPI-Iteration \cite{Liang2015}. It separates the shuffle procedure into fine-grained operations thus permitting the overlap of data movement in an efficient matter, being able to achieve up to 3 times improvement in comparison with Spark's PageRank and K-means algorithms.

The shuffling issue was addressed some time later by the Spark group themselves. \citeauthor{Armbrust2015} \cite{Armbrust2015} recognize the problem of shuffling operations on many nodes, along with some other memory and usability issues. The former affair was resolved by changing the original NIO-based network module with a new implementation based on Netty. The latter problems were specifically that memory management needed to be improved, that performance debugging was hard to do, and that there was some confusion about Spark's API.

The memory management issue was dealt with by adding more caps to take care of compressed data that could scale from a few hundreds of MBs to a couple of GBs. As for the debugging, new metrics were added to the monitoring UI; allowing users to see things such as time taken to schedule, run and receive results from tasks. They also added new data visualization tools to monitor the jobs. The confusion about the API was addressed by developing a more declarative API, one that is based on data frames, such as the ones in Python and R. With this new API, the standard data frame interface is compiled using the optimizer in SparkSQL \cite{armbrust2015spark}. These data frames are also now being used as input and output between Spark's high level libraries. These changes were introduced in Spark's version 1.2.0.

\citeauthor{Nicolae2016} argue that acceleration of shuffling requires the use of memory, which is a precious source and hence its effective utilization is very important \cite{Nicolae2016}. They propose a data transfer strategy that intends to minimize auxiliary memory utilization in the shuffling phase, they achieve it with different design principles such as node-responsiveness based prioritization, load balancing of fetch requests, and static circular allocation of initial requests. That same group later published in \citeyear{nicolae2017leveraging} the results of their design in different shuffle-intensive experiments in a cluster, achieving up to 40\% better performance with 50\% less memory utilization in the shuffle phase \cite{nicolae2017leveraging}.

One early work was to adapt RDMA (Remote Direct Memory Access) to Spark in an InfiniBand Architecture \cite{Lu2014}. The InfiniBand Architecture is ``a new industry-standard architecture for server I/O and inter-server communication'' \cite{pfister2001introduction}. The authors claim that Spark cannot fully use InfiniBand's capabilities to obtain optimal performance, so they adapt RDMA to be a part of the communication infrastructure by overriding Spark's native features so RDMA calls would be used instead of the Java Socket interface. Their adaptation achieved an 83\% performance improvement in the test tasks that they used and conclude that with their RDMA-base design Spark applications could be improved.

This same group continued with this study \cite{Lu2016}: in \citeyear{Lu2016} they tested an advanced RDMA-based design in the changed Apache architecture that came with version 1.2.0 \cite{Armbrust2015}. They particularly check for improvements based on RDMA in the new shuffle architecture based on Netty instead of NIO. They implement an advanced design in three clusters with the most recent InfiniBand technologies and test several workloads on RDD Benchmarks, graph processing and SQL queries on top of Spark scaling up to 1536 cores. The results show that they achieve up to 79\% improvement in RDD performance, 46\% for graph workloads and 32\% for SQL queries. 

Some of the subjects that are left as future work are synchronization across nodes, interference between independent shuffles \cite{nicolae2017leveraging}, optimizing independent shuffles or minimizing interference with other workloads \cite{Nicolae2016}. The Spark group also mentions that they are currently working on memory management outside the JVM and runtime code generation \cite{Armbrust2015}. There also can be more investigation about architectural bottlenecks in Spark and how to improve them \cite{Lu2016}.

\subsection{Improvement based on file system, OS and JVM parameters}

Spark is built in Scala, so it heavily uses the Java Virtual Machine to run tasks, and it also has to run on top of a file system and an operating system. Many clusters nowadays come with some Linux-based OS, and HDFS is a very common file system to use in these clusters. It is natural to try to accelerate the execution of Spark tasks by fine-tuning certain parameters that are based on the OS and file system used, or in the configuration of the JVM.

One of the first efforts was done by \citeauthor{Islam2015} who focused on studying the impacts of two file systems (Tachyon and Triple-H) on Spark applications \cite{Islam2015}. In particular, they identified critical parameters on the performance of the applications that run on top of these file systems, such as BlockSize, concurrent containers and concurrent tasks. They also propose selective caching into high performance memory layers (such as RAMDisks or SSDs). This selective caching refers to caching the data that is the output of an iteration coming from a map or reduce task. One more optimization comes from improving the cache eviction algorithm after each algorithm iteration. They see that CPU-bound workloads do not show as much benefit as I/O intensive ones, and that iterative applications gain performance over the Triple-H file system.

\citeauthor{Wang2016} \cite{Wang2016} study the behaviour of Linux virtual memory to find out the effect of kernel parameters. They test this with Spark workloads that are especially sensitive to memory parameter tuning. They test the effect of transparent huge page but find none. They find that enabling Non-uniform memory access (NUMA) yields better results for some workloads, though not all. The conclusion of this work is that better results can be achieved by tuning certain parameters, although this study was small and not so generalizable.

Since Spark runs on top of Java Virtual Machines (JVMs), another way to improve performance is to tune JVM parameters. \citeauthor{Chiba2016} use different JVM and OS parameters to try to take full advantage of computing resources \cite{Chiba2016}. They argue that Spark creates many objects in heaps, so garbage collection (GC) pause time is a parameter that may cause execution time to be too long. They propose a series of optimizations (such as reducing the nursery space in the heap, changing the number of JVMs in a single node and applying NUMA affinity) and then test them in the TPC-H query benchmark,  which is ``a decision support benchmark [that] consists of a suite of business oriented ad-hoc queries and concurrent data modifications'' \cite{TPC_H_benchmark}. They find that these optimizations performed up to 5 time faster and ran 30\% to 40\% faster on average.
% These guys cite some other papers that do similar work as theirs. Check them out. They also mention some future work on other Spark workloads. See if they continued.

Another similar work was done by \citeauthor{Hema2016}, testing the behaviour of Spark under different JVM parameters (heap size and GC frequency), testing it with machine learning case studies, particularly k-means, matrix factorization and logistic regression \cite{Hema2016}. They found that k-means and logistic regression didn't have performance issues in their tests because heap size was sufficient and it did not need much garbage collection, whilst matrix factorization had a heap memory bottleneck because of the large number of created objects, and consequently GC is called rather frequently. They say that performance can be improved by increasing the executor memory.

\subsection{Improvement based on GPUs}

Another line of improvement is to make use of the power of Graphics Processing Units (GPUs) along with Spark. GPUs have thousands of cores to process parallel workloads efficiently, which can be especially useful for many machine learning applications, so an intention to unite GPUs with Spark cluster computing was only natural.

The earliest work was done by \citeauthor{Li2015} in \citeyear{Li2015}, who proposed combining GPUs with CPU-based cluster computing framework \cite{Li2015}. To do so, they implement HeteroSpark, a framework that augments the usual Spark framework by using GPUs in Spark worker nodes. The function calls from Spark are read by HeteroSpark and directed to the GPU via Java's Remote Method Invocation (RMI). If a function call is not implemented in the GPU, the developer may opt to use the original implementation or decide to implement it themselves. The results show that, not surprisingly, performance is much better with Heterospark. This is because of the GPU acceleration. What the authors remark is that this improvement was achieved without sacrificing neither programmability nor portability.

Another effort was made by \citeauthor{Manzi2016}, who explore the feasibility and potential benefits of combining GPUs and Spark. Since most non-shuffling operations can be completed on GPUs, their implementation focuses on porting these operations to the GPUs \cite{Manzi2016}. Their implementation first has to convert original data to a GPU-compatible format, and their conclusion from the analysis is that ``the performance improvement for any application will depend on how much the gain from GPU thread-level parallelism overshadows the data conversion cost'', e.g., in an implementation for WordCount, the CPU converts from unicode characters to ASCII, and this step takes 57\% of the total time \cite{Manzi2016}. In the K-means algorithm, data shuffling is a minor part of the process, so the gains are much higher; so it is better to use GPUs with algorithms that make many iterations over the same dataset so that the data preparation cost is offset.

\citeauthor{Hassaan2016} introduce DAMB, a system for streaming data in a heterogeneous cluster with GPUs and CPUs, and SparkGPU, its core Spark extension that manages and delegates task in the cluster \cite{Hassaan2016}. Their work is tested in a real-time lightning monitoring station that generates 1.6 GB of data per second and achieve very good results. 

\citeauthor{Ohno2017} accelerate RDD operations for a single computer using GPUs that are either mounted locally or remotely, leveraging how many partitions of RDDs are cached in local memory and how many in remote memory. Their implementation shows a speed-up of up to 21 times compared to Vanilla Spark \cite{Ohno2017}.

Despite these efforts, caching data in GPU memory for iterative processes had not yet been fully explored, until the work of \citeauthor{Hong2017} came along \cite{Hong2017}. They noted that in most of the GPU-accelerated applications came from the system overhead and not from the computations themselves. They present a PySpark based extension for Spark that does efficient GPU in-memory caching and processing. They show that their system improves the performance of different iterative algorithms, such as logistic regression and k-means.


\subsection{Improvement based on performance prediction}

Performance prediction refers to approximating a performance metric (such as execution time or memory consumption) based on the input data fed to the Spark workers. Performance can vary depending on different things such as input data size, type of work done, computing capability, computing architecture, etc. Knowing how much resources a given task will take is useful for the allocation of resources and load balancing in computer clusters.

\citeauthor{wang2015performance} \cite{wang2015performance} present a model that predicts execution time, memory consumption and I/O cost. The model first executes the Spark program on a cluster using a sample of the data, thus collecting information about execution time, memory consumption and I/O costs. They find that their predictions are accurate for execution time and memory consumption, but not so for I/O cost. They conclude that this approach can help better allocate resources in a cluster running Spark.

In \citeyear{Ulanov2017}, \citeauthor{Ulanov2017} develop a model to that captures the processing time of a given machine learning algorithm in a parallelized computing cluster \cite{Ulanov2017}. Their study aims at modeling the scalability of an algorithm so that the modeler can have an idea in advance of how much the task will take in a given cluster architecture.

In a similar line of improvement, \citeauthor{Marco2017} \cite{Marco2017} developed a machine learning algorithm that predicts the memory requirement of any Spark application in order to use effective task co-location strategies. The machine learning model is trained for each specific task by taking a subset of the dataset that is going to be used and then fitting a set of linear and non-linear regressions. Their approach was tested with a range of Spark applications that cover different application domains and show an accurate system modeling model. Because of these accurate predictions, the memory resources are better utilized and thus the system achieves performance improvements by effectively using the runtime task scheduler.

\citeauthor{BrandonHernandez2017} develop an algorithm to predict a good parallelization strategy so that users can have an optimal configuration before starting the cluster work \cite{BrandonHernandez2017}. Their method was tested in 15 different applications and see they can achieve up to 51\% improvement. Since they used simple models such as linear regression and decision trees, their models can be interpreted for recommendations and explanations as how the number of tasks per nodes affects certain workload.

\subsection{Improvements for HPC systems}

Even though Spark is most commonly used in large data centres, it can also be used in an HPC context, with supercomputers, such as Cray supercomputers. In \citeyear{Segal2015} \citeauthor{Segal2015} presented SparkCL \cite{Segal2015}, a framework that intended to bring unconventional computing power such as GPUs, FPGAs, APUs and DSPs usually found in heterogeneous systems into a mainstream programming use (such as Spark), thus avoiding the inconvenience of working with different type of programming languages that come with hardware specialization. To use the framework, the programmer must write an algorithm in Java deriving from SparkCL-based classes and must override and define different support functions and the kernel function that is meant to be accelerated in the cluster. They implemented three functions to test the operation of SparkCL and conclude that their tool can be used through a high-level Java-OpenCL code in a heterogeneous cluster, although they are missing more detailed results such as speed-up of operations.

In \citeyear{Chaimov2016} \citeauthor{Chaimov2016} study the performance of Spark in two Cray XC systems in a large supercomputing centre \cite{Chaimov2016}. They first test the performance of a single workstation with SSDs with a single node of a Cray XC and found that the Cray node was slower. They attributed this to the latency involved in opening files (in \texttt{fopen}). To calibrate the performance in the Cray they propose using a local file system in a Lustre file or main memory and to use pooling when opening files so that metadata is cached.
%They argue that in supercomputer settings, disk I/O is optimized for bandwidth and network is optimized for latency, unlike in data centres where disk I/O is primarily optimized for latency and network is optimized for bandwidth.

HPC clusters are increasingly being equipped with fast high-performance storage, such as SDDs and RAMDisks, and with high-performance interconnects; although not all nodes in the same HPC cluster share the same characteristics: it is common to have SDDs in some nodes and normal HDDs in others. Since the Spark scheduler assumes homogeneity in the nodes, it schedules taking into account the locality of the data, which may not be the best idea for a heterogeneous system \cite{Islam2016}. \citeauthor{Islam2016} consider some approaches to solve the challenge of choosing a criteria for efficient data access and redesign the HDFS to include their strategies, leading to performance improvement compared to the locality based data access in the benchmarks tested \cite{Islam2016}.

Another recent effort that has been made is to combine MPI with Spark. \citeauthor{Morris2017} present MPIgnite \cite{Morris2017}, a framework that introduces peer-to-peer communication concepts into Spark. And the most recent work in the area of improving spark for HPC systems was made by \citeauthor{kim2017sparkle}. In their article they introduce Sparkle \cite{kim2017sparkle}, a library that enhances Spark for scale-up servers, such as HPC systems, leveraging the shared memory these systems have. With shared memory, data shuffling is less inefficient because there is no dependence on data transfer between distant nodes. Vanilla Spark makes the JVM garbage collector work on deleting a lot of objects in the heap memory of the worker nodes, but Sparkle takes care of this by efficiently storing data in the global shared memory. In the performance tests, Sparkle in a scale-up system showed an improvement of 1.6 up to 5 times in comparison to Vanilla Spark running on a scale-out cluster with equivalent hardware, this mainly due to the faster communication between the nodes.

Finally, in \citeyear{Anderson2017}, \citeauthor{Anderson2017} pretend to bridge the gap between Spark and HPC frameworks such as MPI and take the both of both: MPI's performance and Spark's fault tolerance \cite{Anderson2017}. They argue that although Spark has done a very good job in accelerating the performance gotten from using Hadoop alone, it still falls short in performance in comparison with certain HPC environments with native code and optimized communication (e.g., MPI). In their work, they serialize the data from Spark's RDDs and transfer it for MPI processing. They test their framework with four applications and find that they can achieve between 3 and 17 times speedups compared to Spark-based implementations of the same applications. Although they do put an emphasis on the fact that their framework will only make sense for certain operations in which Spark takes longer than some fixed overheads inherent from the data transfer from RDDs to MPI, i.e., operations which have more complex communication.

Although there has been good work in this area, most, if not all, of the improvements mentioned here seem to involve great programming efforts to get the programs running, and each of them have been ad-hoc improvements for certain type of HPC clusters.


\section{Conclusions and future work}
\label{sec:conclusion}

Apache Spark is a widely used framework for data analysis, both in industry and in research, and because of this wide use, there is room for improvement. The big data trend is not stopping any time soon, so there is a need to keep filling holes in data processing technologies. The contributors of Apache Spark continue to make improvements, like extending Spark for other languages like R, creating new high level libraries and enhancing performance \cite{zaharia_apache_2016}; but they cannot cover all the niche areas in which Spark can be used, so there are a lot of efforts to mix different technologies with Vanilla Spark.

Most of the improvements are for particular systems and lack generality; and in general they cannot be so easily integrated with the usual Spark installation. Future work should perhaps be aimed at making more user-friendly, like in high-level libraries, some of the tools presented here, so that less specialized scientists can make use of them without so much burden.

Also, in each of the papers presented here, authors recognize the particular areas in which they can improve their contributions, and most of them are aimed at reducing some sort of overhead related to their adaptations. This is a quite important subject which, even though is very general, it is paramount that if a potential improvement is implemented, the overhead induced by this shall not reduce the benefit that could be obtained by the implementation. For instance, in \cite{Anderson2017} it is mentioned that their implementation has an overhead of several seconds, so it should be used only when the potential benefit is larger than those seconds that are introduced as overhead.

Finally, there are always new developments in computer architecture, so the improvement of Apache Spark is a continuous work that will never be complete because there are always new areas in which it can be polished. 


\printbibliography
%\nocite{*}


\end{document}
